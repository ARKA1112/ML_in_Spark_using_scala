// Databricks notebook source
// MAGIC %md

// COMMAND ----------

// MAGIC %md
// MAGIC ## Project3 -  Market Analysis in Banking Domain
// MAGIC 
// MAGIC ### Author - Arka Prava Panda

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC #### Objective - Your client, a Portuguese banking institution, ran a marketing campaign to convince potential customers to invest in a bank term deposit scheme. 
// MAGIC #### The marketing campaigns were based on phone calls. Often, the same customer was contacted more than once through phone, in order to assess if they would want to subscribe to the bank term deposit or not. You have to ### perform the marketing analysis of the data generated by this campaign.

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q1> Load data and create a Spark data frame
// MAGIC #### At the first step we are uploading the csv in test format and partitioning it into 15 partitions

// COMMAND ----------

val bank_data = sc.textFile("FileStore/Project_1_dataset_bank_full__2_-1.csv",15)

// COMMAND ----------

bank_data.take(2)

// COMMAND ----------

val bank_data1 = bank_data.map(x => x.replace("\"",""))

// COMMAND ----------

val header = bank_data1.first()

// COMMAND ----------

val bank_data2 = bank_data1.filter(x => x != header)

// COMMAND ----------

bank_data2.take(2)

// COMMAND ----------

val bank_data3 = bank_data2.toDF()

// COMMAND ----------

// MAGIC %md
// MAGIC #### Now we are writing this file as a csv in hdfs

// COMMAND ----------

bank_data3.write.csv("bankcsv3.csv")

// COMMAND ----------

// MAGIC %md
// MAGIC #### Then we are again loading this csv designating proper syntax

// COMMAND ----------

val bank_data4 = spark.read.format("csv").option("delimiter",";").load("/bankcsv3.csv")

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC #### Assigning the headers manually and saving it into a file as spark dataframe

// COMMAND ----------

val bank_data5 = bank_data4.toDF("age","job","marital","education","default","balance","housing","loan","contact","day","month","duration","campaign","pdays","previous","poutcome","y")

// COMMAND ----------

// MAGIC %md
// MAGIC #### Checking the the newly created dataframe if everything is in order

// COMMAND ----------

bank_data5.show(10)

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q2>     Give marketing success rate (No. of people subscribed / total no. of entries) and Give marketing failure rate
// MAGIC 
// MAGIC #### Creating a temporary view in order to use it with SparkSQL

// COMMAND ----------

bank_data5.createOrReplaceTempView("bankdf")

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC #### Since the subscription status is in string format I am using a case option in the below sql query to take the yes as 1 and the no as 0 and also the data has no null values, also I have rounded off the result to 2 decimal places

// COMMAND ----------

spark.sql(""" select round(sum(sub)*100/count(sub),2) as success_rate from (select *, case when y = "yes" then 1 else 0 end as sub from bankdf) bnkdf """).show

// COMMAND ----------

// MAGIC %md
// MAGIC #### In the above query one can see that the success rate of subscription is 11.69 % and hence the failure rate will be 100-11.69 ie 88.31%

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q3>Give the maximum, mean, and minimum age of the average targeted customer

// COMMAND ----------

spark.sql("""select max(age) MAX_AGE,round(mean(age),2) AVG_AGE,min(age) MIN_AGE from bankdf""").show()

// COMMAND ----------

// MAGIC %md
// MAGIC #### from the above output we can say that the maximum age, average age and minimum age is 95 years, 40.94 years and 18 years respectively

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q4> Check the quality of customers by checking average balance, median balance of customers

// COMMAND ----------

spark.sql("""select round(mean(balance),2) as AVG_BALANCE, percentile_approx(balance, 0.5) as MEDIAN_BALANCE from bankdf""").show

// COMMAND ----------

// MAGIC %md
// MAGIC ####From the above query we can observe that the average balance is 1362.27 and the median balance is 448.0

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q5> Check if age matters in marketing subscription for deposit

// COMMAND ----------

// MAGIC %md
// MAGIC ##### Now we will create a table which will classify the age into groups 

// COMMAND ----------

val case_table_tot = spark.sql("""select case
when age>=18 and age<=30 then '18-30'
when age>=31 and age<=40 then '31-40'
when age>=41 and age<=50 then '41-50'
when age>=51 and age<=60 then '51-60'
when age>=61 and age<=70 then '61-70'
when age>=71 and age<=80 then '71-80'
when age>=81 and age<=95 then '81-95'
end as age_category_yrs,
count(*) as total_counts
from bankdf
group by
age_category_yrs
order by age_category_yrs""")

// COMMAND ----------


/*checking the table*/
case_table_tot.show

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC ##### now we will create a output which will show how many people subscribed per age category

// COMMAND ----------

val case_table_sub = spark.sql("""select case
when age>=18 and age<=30 then '18-30'
when age>=31 and age<=40 then '31-40'
when age>=41 and age<=50 then '41-50'
when age>=51 and age<=60 then '51-60'
when age>=61 and age<=70 then '61-70'
when age>=71 and age<=80 then '71-80'
when age>=81 and age<=95 then '81-95'
end as age_category_yrs,
count(*) as total_sub_counts
from bankdf
where y = 'yes'
group by
age_category_yrs
order by age_category_yrs""")

// COMMAND ----------

case_table_sub.show

// COMMAND ----------

// MAGIC %md
// MAGIC ##### As the table has been created and stored in a variable called case_table we will create a temporary view out of it in order to query it with spark sql

// COMMAND ----------

case_table_tot.createOrReplaceTempView("case_table_total_df")
case_table_sub.createOrReplaceTempView("case_table_sub_df")

// COMMAND ----------

// MAGIC %md
// MAGIC ##### Now we will join the two tables and compare their subscription rates

// COMMAND ----------

spark.sql("""select s.age_category_yrs,s.total_sub_counts,t.total_counts, round(s.total_sub_counts*100/t.total_counts,2) as per_sum
from case_table_total_df t
inner join case_table_sub_df s on
t.age_category_yrs = s.age_category_yrs
order by t.age_category_yrs""").show

// COMMAND ----------

// MAGIC %md
// MAGIC ##### from the above table we can observe that people in the age range of 61-95 yrs has a higher subscription rate but lower number of total subscriptions

// COMMAND ----------

// MAGIC %md
// MAGIC ##### Now we will query what is the age_category and how much percentage of the total subscription it contributes to

// COMMAND ----------

spark.sql("""select *,
round(sum(total_sub_counts) over(order by age_category_yrs) * 100/sum(total_sub_counts) over (),2) cumsum 
from case_table_sub_df""").show

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC ### From the above table we can see the rolling percentage sum in the per_sum column and can also infer the following
// MAGIC 
// MAGIC ###### 1) The age_categories of 18-60 years contributes to over 90% of the successful subscription
// MAGIC ###### 2) Also the age_categories of 18-50 years contribute to over 75% of the successful subscription
// MAGIC ###### 3) Finally the age_categories of  18-40 years contribute to over 55% of the successful subscription

// COMMAND ----------

// MAGIC %md
// MAGIC ## Q5> Check if marital status mattered for a subscription to deposit

// COMMAND ----------

spark.sql("""with subs as (select marital, count(*) tot_sub_count from bankdf where y = 'yes' group by marital),
tot as (select marital, count(*) tot_count from bankdf group by marital)
select subs.marital, subs.tot_sub_count, tot.tot_count,
round(subs.tot_sub_count*100/tot.tot_count,2) per_sub
from tot
inner join subs on
subs.marital = tot.marital""").show

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC #### As from the above query we can see total number respondents who subscribed, total count of the respondents and the percentage of them who subscribed
// MAGIC ##### We can also infer the following
// MAGIC ###### There is no significant difference among the percentage of respondents who subscribed across all the marital statuses although the subscription rate of the marital status: single is a little higher
// MAGIC ###### Also among all the subscribers the single and the married respondents contribute to the major part of the subscription

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q6> Check if age and marital status together mattered for a subscription to deposit scheme

// COMMAND ----------

val age_cat_table = spark.sql("""select*, case
when age>=18 and age<=30 then '18-30'
when age>=31 and age<=40 then '31-40'
when age>=41 and age<=50 then '41-50'
when age>=51 and age<=60 then '51-60'
when age>=61 and age<=70 then '61-70'
when age>=71 and age<=80 then '71-80'
when age>=81 and age<=95 then '81-95'
end as age_category_yrs
from bankdf
order by age_category_yrs""").show(10)

// COMMAND ----------

age_cat_table.createOrReplaceTempView("age_cat_table_df")

// COMMAND ----------

val age_marriage = spark.sql("""select '18-30' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '18-30' group by marital
union
select '31-40' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '31-40' group by marital
union
select '41-50' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '41-50' group by marital
union
select '51-60' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '51-60' group by marital
union
select '61-70' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '61-70' group by marital
union
select '71-80' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '71-80' group by marital
union
select '81-95' age_cat,marital,count(y) from age_cat_table_df
where y='yes' and age_category_yrs = '81-95' group by marital
""")

// COMMAND ----------

age_marriage.createOrReplaceTempView("age_marriage_df")

// COMMAND ----------

spark.sql("""select *  from age_marriage_df where marital='single'""").show

// COMMAND ----------

spark.sql("""select * from age_marriage_df where marital='divorced'""").show

// COMMAND ----------

spark.sql("""select * from age_marriage_df where marital='married'""").show

// COMMAND ----------

// MAGIC %md
// MAGIC #### The above three queries show the following:
// MAGIC ##### age category (18-30 yrs) followed by (31-40 yrs) and (41-50 yrs) in the 'single' marital category contributed to the major subscription
// MAGIC ##### age category (31-40 yrs) followd by (41-50 yrs) and (51-60 yrs) in the 'married' marital categrory contribute to the major subscription
// MAGIC ##### subscriptions in the 'divorced' marital status is significantly low than the 'single' and 'married' categories.
// MAGIC ##### We can say that age along with marital status mattered for a subscription to deposit scheme.

// COMMAND ----------

// MAGIC %md
// MAGIC ### Q7> Do feature engineering for the bank and find the right age effect on the campaign

// COMMAND ----------

spark.sql("""select *, case
when age>=18 and age<=30 then '18-30'
when age>=31 and age<=40 then '31-40'
when age>=41 and age<=50 then '41-50'
when age>=51 and age<=60 then '51-60'
when age>=61 and age<=70 then '61-70'
when age>=71 and age<=80 then '71-80'
when age>=81 and age<=95 then '81-95'
end as age_category_yrs
from bankdf
order by age_category_yrs""").show(10)

// COMMAND ----------

spark.sql("""select age_category_yrs, count(*) as subs from age_cat_table_df where y ='yes' group by age_category_yrs order by age_category_yrs""").show

// COMMAND ----------

// MAGIC %md
// MAGIC #### We can see that the age_categories  of (18-30 yrs),(31-40 yrs),(41-50 yrs),(51-60 yrs) consists of highest subscribers.
// MAGIC ##### With highest subscriptions coming from the age_category  of (31 - 40 yrs)

// COMMAND ----------


